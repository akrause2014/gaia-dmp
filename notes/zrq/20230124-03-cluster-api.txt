#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        See what we can do with StackHPC's Helm charts ..

    Result:

        Work in progress ...

# -----------------------------------------------------

    Start with StackHPC's Helm charts.
    https://github.com/stackhpc/capi-helm-charts

        The openstack-cluster chart depends on features in cluster-api-provider-openstack that are not yet in a release.

        StackHPC maintain custom builds of cluster-api-provider-openstack for use with these charts.
        You can find these in the StackHPC fork of cluster-api-provider-openstack.

    StackHPC's fork of the OpenStack Cluster-API provider
    https://github.com/stackhpc/cluster-api-provider-openstack

        Packages
        https://github.com/orgs/stackhpc/packages?repo_name=cluster-api-provider-openstack

            capi-openstack-controller
            docker pull ghcr.io/stackhpc/capi-openstack-controller:v0.7.0-stackhpc.3

             capi-openstack-controller-amd64
             docker pull ghcr.io/stackhpc/capi-openstack-controller-amd64:v0.7.0-stackhpc.3


# -----------------------------------------------------

    Back to StackHPC's Helm charts.
    https://github.com/stackhpc/capi-helm-charts

        Prerequisites

            First, you must set up a Cluster API management cluster with the OpenStack Infrastructure Provider installed.

            WARNING
            This chart depends on features in cluster-api-provider-openstack that are not yet in a release.
            StackHPC maintain custom builds of cluster-api-provider-openstack for use with this chart. You can find these in the StackHPC fork of cluster-api-provider-openstack.

            Addons are managed by the Cluster API Addon Provider, which must also be installed if you wish to use the addons functionality.

            In addition, Helm must be installed and configured to access your management cluster, and the chart repository containing this chart must be configured:
                helm repo add capi https://stackhpc.github.io/capi-helm-charts

    Management cluster
    https://cluster-api.sigs.k8s.io/user/concepts.html#management-cluster

        A Kubernetes cluster that manages the lifecycle of Workload Clusters.
        A Management Cluster is also where one or more providers run, and where resources such as Machines are stored.

    Infrastructure provider
    https://cluster-api.sigs.k8s.io/user/concepts.html#infrastructure-provider

        A component responsible for the provisioning of infrastructure/computational resources required by the Cluster
        or by Machines (e.g. VMs, networking, etc.). For example, cloud Infrastructure Providers include AWS, Azure,
        and Google, and bare metal Infrastructure Providers include VMware, MAAS, and metal3.io.

# -----------------------------------------------------

    Backing up a few steps ...

    Cluster-API Quick start
    https://cluster-api.sigs.k8s.io/user/quick-start.html

        Install and setup kubectl in your local environment

            Native RedHat package manager
            https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
sudo yum install -y kubectl

        Install kind and Docker

            Kind
            https://kind.sigs.k8s.io/

            If you have go (1.17+) and docker installed all you need is :

                ```
                go install sigs.k8s.io/kind@v0.17.0 && kind create cluster
                ```

            Starting with kind 0.11.0, Rootless Docker and Rootless Podman can be used as the node provider of kind.

                cgroup v2 is enabled by default on Fedora. On other distros, cgroup v2 can be typically enabled
                by adding GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=1" to /etc/default/grub and running
                sudo update-grub.

            Also, depending on the host configuration, the following steps might be needed:

                Create /etc/systemd/system/user@.service.d/delegate.conf with the following content:

                    ```
                    [Service]
                    Delegate=yes
                    ```

                and then run

                    ```
                    sudo systemctl daemon-reload
                    ```

                This is not enabled by default because “the runtime impact of [delegating the “cpu” controller] is still too high”.
                Beware that changing this configuration may affect system performance.

                Create /etc/modules-load.d/iptables.conf with the following content:

                    ```
                    ip6_tables
                    ip6table_nat
                    ip_tables
                    iptable_nat
                    ```


            To create a kind cluster with Rootless Podman, just run:

                ```
                export KIND_EXPERIMENTAL_PROVIDER=podman
                kind create cluster
                ```

            On some distributions, you might need to use systemd-run to start kind into its own cgroup scope:

                ```
                systemd-run --scope --user kind create cluster
                ```

# -----------------------------------------------------

    Delete everything

    Create an initial VM, using Openstack command line.

    Install kubectl

    Install kind

    Use kind to create a cluster



# -----------------------------------------------------
# Check which cloud is currently live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Tue 24 Jan 12:37:36 UTC 2023
    >   iris-gaia-blue-20221013-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is blue, selecting green for experimenting.
    #

    source "${HOME:?}/aglais.env"

    agcolour=green
    configname=zeppelin-54.86-spark-6.26.43

    agproxymap=3000:3000
    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --publish  "${agproxymap:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "configname=${configname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m30.475s
    >   user    1m7.350s
    >   sys     0m7.316s

    #
    # Still some undeletable shares.
    #

    >   ....
    >   ---- ----
    >   List shares
    >   +--------------------------------------+---------------------------------------+------+-------------+----------+-----------+-----------------+------+-------------------+
    >   | ID                                   | Name                                  | Size | Share Proto | Status   | Is Public | Share Type Name | Host | Availability Zone |
    >   +--------------------------------------+---------------------------------------+------+-------------+----------+-----------+-----------------+------+-------------------+
    >   | 5163ec64-038b-4a9d-9620-5ab8eaac6173 | iris-gaia-green-home-Zoh0seedie0hieRu |    1 | CEPHFS      | deleting | False     | ceph01_cephfs   |      | nova              |
    >   | b6d057b0-3819-40ef-b41f-846c5ebe33e8 | iris-gaia-green-user-Zoh0seedie0hieRu |    1 | CEPHFS      | deleting | False     | ceph01_cephfs   |      | nova              |
    >   +--------------------------------------+---------------------------------------+------+-------------+----------+-----------+-----------------+------+-------------------+
    >   ....




